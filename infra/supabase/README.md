# Supabase Database Schema

Database schema for Neura with pgvector support powering retrieval augmented generation (RAG) and spaced repetition flashcards. Provides multi-tenant isolation via Row Level Security (RLS).

## Overview

Features:
- pgvector extension for 1536-dim embedding storage (Gemini-compatible)
- IVFFLAT cosine index for fast similarity search
- Multi-tenant RLS policies across all tables
- Spaced repetition fields (SM-2) in `flashcards`
- Cascading relationships for automatic cleanup

Flow: upload → extract → embed → chat (RAG) → generate flashcards → practice

## Storage Buckets

Object storage buckets used by the application:
- `uploads/` — User-uploaded files (PDF, DOCX, TXT, PNG, JPG)
- `processed/` — AI-generated content (notes, mindmaps, exports)

Both buckets are non-public with RLS policies that restrict access to paths whose
first folder matches the authenticated user's id: `bucket/{user_id}/...`.
Signed URLs are generated by the backend to provide short-lived download links.

## Tables

1. `profiles` – 1:1 with `auth.users` (basic user metadata)
2. `documents` – Uploaded files + extracted text + processing status
3. `embeddings` – Chunked text + embedding vectors for similarity search
4. `flashcards` – Q&A items with SM-2 scheduling fields
5. `match_embeddings` (RPC function) – Cosine similarity search for RAG chat context

Relationship sketch:
```
auth.users
    ↓ 1:1
profiles
    ↓ 1:N
documents
    ↓ 1:N
embeddings  (used for similarity search)
    ↘ optional N:1
flashcards (reference document for provenance)
```

## Key Design Decisions
- pgvector + IVFFLAT (`lists=100`) gives ~10x speedup for similarity over sequential scan; tune lists ≈ sqrt(row_count)
- Cascading deletes maintain referential integrity (delete document → embeddings & dependent flashcards removed/set null)
- RLS ensures users only access their own rows regardless of application bugs
- Service role key bypasses RLS for backend processing tasks (embedding generation, maintenance)
- Document `status` column provides pipeline state: `uploaded | extracted | embedded | failed`
- SM-2 fields (`efactor`, `repetitions`, `interval`, `next_review`) enable native scheduling without extra tables

## Migration Instructions

Option 1: Migration script (raw SQL via psql)
```bash
cd infra/supabase
chmod +x migrate.sh
./migrate.sh          # Apply schema + RLS (requires SUPABASE_DB_URL)
./migrate.sh --seed   # + seed data (development only)
```
Requires environment variables in `.env`:

```
Note: The seed script expects a test auth user with id `00000000-0000-0000-0000-000000000001`. Create a user via Supabase Auth (email/password or magic link) and, if needed for local testing, update the seed UUID or create a user with that UUID using Admin APIs.
SUPABASE_URL=https://<ref>.supabase.co
SUPABASE_SERVICE_ROLE_KEY=...  # used for authentication
SUPABASE_DB_URL=postgresql://<user>:<password>@db.<ref>.supabase.co:5432/postgres
```
Important: The `<password>` in `SUPABASE_DB_URL` is the Postgres database password from the Supabase dashboard (Settings → Database). Do NOT use the service role key here; that key is only for API calls and bypassing RLS at the REST/JS layer. Rotate the DB password periodically via the dashboard and update `.env`.

Option 2: Supabase CLI native migrations (recommended for production)
```bash
cd infra/supabase
supabase link --project-ref <your-project-ref>
supabase db push
```

Option 3: Manual SQL (psql)
```bash
psql "postgresql://postgres:[password]@db.[project-ref].supabase.co:5432/postgres" -f schema.sql
psql "postgresql://postgres:[password]@db.[project-ref].supabase.co:5432/postgres" -f rls_policies.sql
psql "postgresql://postgres:[password]@db.[project-ref].supabase.co:5432/postgres" -f storage_buckets.sql
```

## Verification
```sql
-- Extension
select * from pg_extension where extname = 'vector';
-- Tables
select tablename from pg_tables where schemaname = 'public';
-- Embedding indexes
select indexname, indexdef from pg_indexes where tablename = 'embeddings';
-- RLS enabled flags
select tablename, rowsecurity from pg_tables where schemaname = 'public';
-- Policies
select * from pg_policies;
```

## Usage Examples
Insert document:
```sql
insert into documents (user_id, filename, storage_path, mime_type, size_bytes)
values ('user-uuid', 'report.pdf', 'uploads/user-uuid/doc-uuid.pdf', 'application/pdf', 1024000);
```
Insert embedding:
```sql
insert into embeddings (document_id, chunk_index, chunk_text, embedding)
values ('doc-uuid', 0, 'This is the first chunk...', '[0.1,0.2,...,0.0]'::vector(1536));
```
Similarity search (cosine distance operator `<=>`):
```sql
select chunk_text,
       1 - (embedding <=> '[query-vector]'::vector(1536)) as similarity
from embeddings
where document_id = 'doc-uuid'
order by embedding <=> '[query-vector]'::vector(1536)
limit 5;
```
Due flashcards for a user:
```sql
select * from flashcards
where user_id = 'user-uuid' and next_review <= now()
order by next_review asc
limit 10;
```

## Vector Similarity Search (Sprint 3)

`match_embeddings()` exposes pgvector cosine similarity search as an RPC function for the backend.

**Function Signature**
```sql
match_embeddings(
    query_embedding vector(1536),
    target_document_id uuid,
    match_count int DEFAULT 5,
    similarity_threshold float DEFAULT 0.0
)
RETURNS TABLE (
    id bigint,
    document_id uuid,
    chunk_index int,
    chunk_text text,
    similarity float
)
```

**Usage Example**
```sql
-- Find top 5 chunks within a document with similarity ≥ 0.3
select * from match_embeddings(
    '[0.1, 0.2, ..., 0.0]'::vector(1536),
    'doc-uuid'::uuid,
    5,
    0.3
);
```

**From Python (Supabase client)**
```python
result = supabase.rpc(
        "match_embeddings",
        {
                "query_embedding": query_embedding,  # List[float] with 1536 dims
                "target_document_id": str(document_id),
                "match_count": 5,
                "similarity_threshold": 0.3,
        },
).execute()

for row in result.data:
        print(f"Similarity: {row['similarity']:.2f}")
        print(row["chunk_text"])
```

**How It Works**
- `<=>` computes cosine distance (0 = identical, 2 = opposite)
- Similarity is derived as `1 - distance` → score in `[0, 1]`
- Results ordered by ascending distance (= descending similarity)
- IVFFLAT index on `embeddings.embedding` accelerates the search automatically
- Filtering by `target_document_id` keeps search scoped to a single document

**Performance**
- With IVFFLAT `lists=100`: ~10-50ms for ~10K embeddings
- Without index: ~500-1000ms (sequential scan)
- Tune `lists` (≈ √row_count) as datasets grow

## Maintenance
Rebuild IVFFLAT with more lists (dataset growth):
```sql
drop index idx_embeddings_vector;
create index idx_embeddings_vector on embeddings using ivfflat (embedding vector_cosine_ops) with (lists = 200);
```
Stats refresh:
```sql
vacuum analyze embeddings;
```
Monitor index usage:
```sql
select * from pg_stat_user_indexes where indexrelname = 'idx_embeddings_vector';
```

## Troubleshooting
- Extension missing: enable `vector` in Supabase Dashboard → Database → Extensions
- Slow search: confirm IVFFLAT index exists; adjust `lists`; run `vacuum analyze`
- RLS denied: check JWT user id matches profile id; inspect `pg_policies`
- Foreign key errors: insert parent rows first (profiles → documents → embeddings/flashcards)

## Storage Buckets (Sprint 1, 4+)

Supabase Storage provides object storage with RLS. Two buckets are configured via `storage_buckets.sql`:

Bucket: uploads/
- Purpose: User-uploaded files (PDF, DOCX, TXT, PNG, JPG)
- Path pattern: `uploads/{user_id}/{document_id}.{ext}`
- Public: false (requires authentication)
- Policies: Users can view/insert/delete only within their own `{user_id}` folder

Bucket: processed/
- Purpose: AI-generated content (notes, mindmaps, flashcard exports)
- Path pattern: `processed/{user_id}/{filename}`
- Public: false (requires authentication)
- Policies: Users can select/insert/update/delete only within their own `{user_id}` folder
- Upsert enabled: Overwrite existing objects for idempotent regeneration

RLS Pattern:
- Policies use `storage.foldername(name)` to extract the first folder of the object key
- Example check: `(storage.foldername(name))[1] = auth.uid()::text`

Signed URLs:
- Generated on-demand by the backend with `create_signed_url(path, expires_in)`
- Typical TTL: 60 seconds

## Chat History Persistence (Optional - Sprint 6+)

The `conversations` table enables persistent storage of RAG chat interactions for document exports and chat history viewing.

**Table Schema**
```sql
create table if not exists conversations (
    id uuid primary key default gen_random_uuid(),
    user_id uuid not null references auth.users(id) on delete cascade,
    document_id uuid not null references documents(id) on delete cascade,
    role text not null check (role in ('user', 'assistant')),
    content text not null,
    created_at timestamptz not null default now()
);

-- Indexes for efficient retrieval
create index if not exists idx_conversations_document_created 
    on conversations(document_id, created_at);
create index if not exists idx_conversations_user 
    on conversations(user_id);
```

**Row Level Security**
```sql
alter table conversations enable row level security;

-- Users can view their own chat history
create policy "Users can view own conversations"
    on conversations for select
    using (auth.uid() = user_id);

-- Users can insert their own messages
create policy "Users can insert own conversations"
    on conversations for insert
    with check (auth.uid() = user_id);

-- Users can delete their own chat history
create policy "Users can delete own conversations"
    on conversations for delete
    using (auth.uid() = user_id);
```

**Integration with Export Endpoint**
- The `/export` endpoint (Sprint 6) queries `conversations` to include "Chat History" section in exports
- If the table does not exist, export gracefully skips the section
- Each conversation row represents a single message turn (user question or assistant response)
- Chat history is ordered by `created_at` ascending in exports

**Enabling the Table**
Apply the optional migration:
```bash
cd infra/supabase
psql "$SUPABASE_DB_URL" -f conversations.sql
```

Or use the migration script (automatically applies if `conversations.sql` exists):
```bash
./migrate.sh
```

**Retention & Cleanup**
- Chat history is deleted when the parent document is deleted (`on delete cascade`)
- Consider adding a retention policy for long-term storage management:
```sql
-- Example: Delete chat history older than 90 days
delete from conversations where created_at < now() - interval '90 days';
```

**Storage Considerations**
- Average message size: ~500 bytes (user question) to ~2KB (assistant response)
- 1000 messages ≈ 1-2 MB storage
- Index on `(document_id, created_at)` enables fast chronological retrieval for exports

## Future Enhancements
- `notes` / `mindmaps` for generated study materials (Sprint 4 ✓ — now using Storage instead)
- `usage_tracking` for quotas & limits
- Full-text search on `extracted_text`
- Partition `embeddings` by document for very large datasets
- Chat history analytics (message count, response time tracking)

## References
- pgvector: https://github.com/pgvector/pgvector
- Supabase RLS: https://supabase.com/docs/guides/auth/row-level-security
- SM-2 Algorithm: https://en.wikipedia.org/wiki/SuperMemo#SM-2_algorithm
