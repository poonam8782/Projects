"""
Content Generation Endpoints

Provides endpoints to transform documents into structured learning materials such
as notes, mindmaps, and flashcards. Sprint 4 implements notes and mindmaps generation,
Sprint 5 implements flashcards generation using the Gemini API.
"""

import json
import logging
import re
import time
from datetime import datetime, timezone
from uuid import UUID

import bleach
from bleach.css_sanitizer import CSSSanitizer
from fastapi import APIRouter, HTTPException, Depends, status, Query

from app.supabase_client import get_supabase_client
from app.core.auth import require_user
from app.schemas import (
    GenerateNotesResponse,
    GetNotesResponse,
    GenerateMindmapResponse,
    GenerateFlashcardsResponse,
    FlashcardResponse,
)
from app.services.gemini_client import (
    generate_notes,
    generate_mindmap,
    generate_mermaid_mindmap,
    generate_markmap,
    generate_flashcards,
)
from app.utils.sm2 import DEFAULT_EFACTOR, DEFAULT_REPETITIONS, DEFAULT_INTERVAL
from app.utils.storage_paths import get_notes_path

# Constants
NOTES_BUCKET = "processed"
SIGNED_URL_EXPIRY = 60  # seconds

# SVG Sanitization allowlists (strict)
ALLOWED_SVG_TAGS = {
    "svg", "g", "rect", "circle", "ellipse", "line", "polyline", "polygon", "path", "text", "tspan",
    "defs", "lineargradient", "radialgradient", "stop", "title", "desc"
}

ALLOWED_SVG_ATTRIBUTES = {
    "*": ["id", "class", "style"],
    "svg": ["width", "height", "viewbox", "xmlns", "version"],
    "g": ["transform"],
    "rect": ["x", "y", "width", "height", "rx", "ry", "fill", "stroke", "stroke-width", "transform"],
    "circle": ["cx", "cy", "r", "fill", "stroke", "stroke-width", "transform"],
    "ellipse": ["cx", "cy", "rx", "ry", "fill", "stroke", "stroke-width", "transform"],
    "line": ["x1", "y1", "x2", "y2", "stroke", "stroke-width", "transform"],
    "polyline": ["points", "fill", "stroke", "stroke-width", "transform"],
    "polygon": ["points", "fill", "stroke", "stroke-width", "transform"],
    "path": ["d", "fill", "stroke", "stroke-width", "transform"],
    "text": ["x", "y", "dx", "dy", "text-anchor", "font-size", "font-family", "fill", "transform"],
    "tspan": ["x", "y", "dx", "dy", "text-anchor", "font-size", "font-family", "fill"],
    "lineargradient": ["id", "x1", "y1", "x2", "y2", "gradientunits"],
    "radialgradient": ["id", "cx", "cy", "r", "fx", "fy", "gradientunits"],
    "stop": ["offset", "stop-color", "stop-opacity"],
    "title": [],
    "desc": [],
}

ALLOWED_SVG_STYLES = [
    "fill", "stroke", "stroke-width", "font-size", "font-family", "text-anchor", "opacity"
]


def sanitize_svg(svg_content: str) -> str:
    """Sanitize SVG content using bleach allowlist approach to prevent XSS.

    Removes any tags/attributes not explicitly allowed, strips scripts, event handlers,
    and external resource references. Ensures resulting SVG still contains <svg> root.

    Args:
        svg_content: Raw SVG string generated by Gemini (untrusted).

    Returns:
        Sanitized SVG string safe for storage/display (still re-sanitize client-side).

    Raises:
        RuntimeError: If sanitization fails or result invalid/empty.
    """
    try:
        # Capture original viewBox (case-insensitive) for later restoration if stripped
        viewbox_match = re.search(r"(?i)viewBox=['\"]([^'\"]+)['\"]", svg_content)
        original_viewbox = viewbox_match.group(1) if viewbox_match else None
        # Normalize camelCase attributes to lowercase variants expected in allowlist
        normalized = re.sub(r"(?i)\bviewBox=", "viewbox=", svg_content)
        normalized = re.sub(r"(?i)\bgradientUnits=", "gradientunits=", normalized)
        css_sanitizer = CSSSanitizer(allowed_css_properties=ALLOWED_SVG_STYLES)
        cleaned = bleach.clean(
            normalized,
            tags=ALLOWED_SVG_TAGS,
            attributes=ALLOWED_SVG_ATTRIBUTES,
            strip=True,
            css_sanitizer=css_sanitizer,
        )
        # Re-inject viewbox attribute if it existed originally and was stripped
        if original_viewbox and "viewbox=" not in cleaned.lower():
            cleaned = re.sub(
                r"<svg(\s|>)",
                lambda m: f'<svg viewbox="{original_viewbox}"' + (" " if m.group(1).isspace() else ">"),
                cleaned,
                count=1,
            )
        # Remove any lingering on* event handlers in style or attributes (defense in depth)
        cleaned = re.sub(r"\son[a-zA-Z]+=\".*?\"", "", cleaned)
        # Ensure no script tags remain
        cleaned = re.sub(r"<script.*?>.*?</script>", "", cleaned, flags=re.IGNORECASE | re.DOTALL)
        # Basic validation
        if not cleaned.strip() or "<svg" not in cleaned.lower():
            raise RuntimeError("SVG sanitization produced empty or invalid output")
        return cleaned
    except Exception as exc:  # noqa: BLE001
        raise RuntimeError(f"SVG sanitization failed: {exc}") from exc

# Router setup
router = APIRouter(prefix="/generate", tags=["generate"])

# Logger setup
logger = logging.getLogger(__name__)


@router.post("/notes", response_model=GenerateNotesResponse)
def generate_notes_endpoint(
    document_id: UUID,
    user: dict = Depends(require_user),
):
    """
    Generate structured markdown notes from a document's extracted text using Gemini AI.

    - Verifies document ownership
    - Validates that extracted_text exists
    - Calls Gemini to generate markdown study notes (non-streaming)
    - Uploads the markdown to Supabase Storage (processed bucket)
    - Returns a short preview and a signed download URL
    """
    user_id = user["sub"]
    start_time = time.time()

    try:
        supabase = get_supabase_client()

        # Fetch document and verify ownership
        try:
            response = (
                supabase.table("documents")
                .select("extracted_text, filename, status")
                .eq("id", str(document_id))
                .eq("user_id", user_id)
                .limit(1)
                .execute()
            )
        except Exception as e:
            logger.error("Database query failed when fetching document %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to fetch document",
            )

        if not response.data:
            logger.warning("Document %s not found for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied",
            )

        doc = response.data[0]
        extracted_text = doc.get("extracted_text") or ""
        filename = doc.get("filename") or str(document_id)

        # Validate extracted text
        if not extracted_text.strip():
            logger.info("Document %s has no extracted text for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Document has no extracted text. Upload and extract text first.",
            )

        # Generate notes via Gemini
        logger.info("Generating notes for document %s", document_id)
        try:
            notes_markdown = generate_notes(
                extracted_text,
                temperature=0.3,
                max_output_tokens=4096,
            )
        except RuntimeError as e:
            logger.error("Gemini notes generation failed for %s: %s", document_id, e)
            msg = str(e).lower()
            if any(k in msg for k in ("rate limit", "quota", "429")):
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Failed to generate notes: {e}",
                )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate notes: {e}",
            )

        if not notes_markdown or not notes_markdown.strip():
            logger.error("Generated notes are empty for document %s", document_id)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Generated notes are empty",
            )

        # Prepare storage path and filename
        # Ensure filename uniqueness by including document_id
        notes_filename = f"{document_id}-notes.md"
        storage_path = get_notes_path(user_id, document_id)
        notes_bytes = notes_markdown.encode("utf-8")

        # Upload to Supabase Storage (upsert=True for idempotency)
        try:
            # Supabase storage client requires string values for all file_options
            # Align with upload.py pattern (uses 'content-type' and 'upsert' as strings)
            upload_response = supabase.storage.from_(NOTES_BUCKET).upload(
                path=storage_path,
                file=notes_bytes,
                file_options={
                    "content-type": "text/markdown",
                    "upsert": "true",  # must be string, not boolean
                },
            )
            if hasattr(upload_response, "error") and upload_response.error:
                logger.error("Storage upload returned error: %s", upload_response.error)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Failed to upload notes to storage",
                )
        except HTTPException:
            raise
        except Exception as e:
            logger.error("Failed to upload notes to storage for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to upload notes to storage",
            )

        # Create signed download URL
        try:
            url_response = supabase.storage.from_(NOTES_BUCKET).create_signed_url(
                storage_path, expires_in=SIGNED_URL_EXPIRY
            )
            # Supabase-py returns dict with 'signedURL' or similar shape
            signed_url = None
            if isinstance(url_response, dict):
                signed_url = url_response.get("signedURL") or url_response.get("signedUrl") or url_response.get("signed_url")
            else:
                signed_url = getattr(url_response, "signedURL", None)
            if not signed_url:
                raise ValueError("Signed URL missing in response")
        except Exception as e:
            logger.error("Failed to generate signed URL for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate download URL",
            )

        processing_time = time.time() - start_time
        logger.info(
            "Notes generation for document %s completed in %.2fs",
            document_id,
            processing_time,
        )

        content_preview = notes_markdown[:500]
        return GenerateNotesResponse(
            document_id=document_id,
            filename=notes_filename,
            storage_path=storage_path,
            download_url=signed_url,
            content_preview=content_preview,
            size_bytes=len(notes_bytes),
            status="success",
            message="Notes generated successfully",
            processing_time_seconds=processing_time,
        )

    except HTTPException:
        # Pass through HTTP errors as-is
        raise
    except Exception as e:  # noqa: BLE001
        logger.error("Unexpected error in /generate/notes: %s", e, exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred",
        )


@router.get("/notes/{document_id}", response_model=GetNotesResponse, tags=["generate"])
def get_notes(
    document_id: UUID,
    user: dict = Depends(require_user),
) -> GetNotesResponse:
    """
    Retrieve the full notes content from storage for a document.

    Fetches previously generated markdown study notes from Supabase Storage.
    Requires the notes to have been generated via POST /generate/notes first.

    Args:
        document_id: UUID of the document
        user: Authenticated user from JWT token

    Returns:
        GetNotesResponse with full markdown content

    Raises:
        404: Document not found, access denied, or notes not generated yet
        500: Storage access failure
    """
    user_id = user["sub"]

    try:
        supabase = get_supabase_client()

        # Verify document ownership
        try:
            response = (
                supabase.table("documents")
                .select("id")
                .eq("id", str(document_id))
                .eq("user_id", user_id)
                .limit(1)
                .execute()
            )
        except Exception as e:  # noqa: BLE001
            logger.error("Database query failed when fetching document %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to fetch document",
            )

        if not response.data:
            logger.warning("Document %s not found for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied",
            )

        # Construct storage path
        notes_filename = f"{document_id}-notes.md"
        storage_path = get_notes_path(user_id, document_id)

        # Download notes from storage
        try:
            download_response = supabase.storage.from_(NOTES_BUCKET).download(storage_path)
            if not download_response:
                logger.warning("Notes file not found at %s for document %s", storage_path, document_id)
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Notes not generated yet",
                )
        except HTTPException:
            raise
        except Exception as e:  # noqa: BLE001
            # Check for not-found condition
            error_message = str(e).lower()
            if "not found" in error_message or "404" in error_message or "does not exist" in error_message:
                logger.warning("Notes file not found at %s for document %s: %s", storage_path, document_id, e)
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Notes not generated yet",
                )
            # Genuine storage failure
            logger.error("Failed to download notes from storage for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to retrieve notes from storage",
            )

        # Decode bytes to UTF-8 string
        try:
            content = download_response.decode("utf-8")
        except Exception as e:  # noqa: BLE001
            logger.error("Failed to decode notes content for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to decode notes content",
            )

        size_bytes = len(download_response)

        return GetNotesResponse(
            document_id=document_id,
            filename=notes_filename,
            content=content,
            size_bytes=size_bytes,
            status="success",
            message="Notes retrieved successfully",
        )

    except HTTPException:
        raise
    except Exception as e:  # noqa: BLE001
        logger.error("Unexpected error in GET /generate/notes/%s: %s", document_id, e, exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred",
        )


@router.post("/mindmap", response_model=GenerateMindmapResponse)
def generate_mindmap_endpoint(
    document_id: UUID,
    format: str = Query("mermaid", regex="^(svg|mermaid|markmap)$"),
    user: dict = Depends(require_user),
):
    """Generate mindmap from a document's extracted text using Gemini AI.

    Supports multiple formats:
    - svg: Traditional SVG mindmap with custom layout (sanitized)
    - mermaid: Mermaid.js mindmap syntax (recommended, best quality)
    - markmap: Markdown-based mindmap format

    Follows same pattern as notes generation: ownership verification, text validation,
    Gemini non-streaming generation, storage upload (upsert), signed URL creation.
    """
    user_id = user["sub"]
    start_time = time.time()

    try:
        supabase = get_supabase_client()

        # Fetch document and verify ownership
        try:
            response = (
                supabase.table("documents")
                .select("extracted_text, filename, status")
                .eq("id", str(document_id))
                .eq("user_id", user_id)
                .limit(1)
                .execute()
            )
        except Exception as e:  # noqa: BLE001
            logger.error("Database query failed when fetching document %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to fetch document",
            )

        if not response.data:
            logger.warning("Document %s not found for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied",
            )

        doc = response.data[0]
        extracted_text = doc.get("extracted_text") or ""

        # Validate extracted text
        if not extracted_text.strip():
            logger.info("Document %s has no extracted text for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Document has no extracted text. Upload and extract text first.",
            )

        # Generate mindmap based on requested format
        logger.info("Generating %s mindmap for document %s", format, document_id)
        try:
            if format == "mermaid":
                raw_content = generate_mermaid_mindmap(
                    extracted_text,
                    temperature=0.3,
                    max_output_tokens=4096,
                )
                content_type = "text/plain"
                file_extension = "mmd"
            elif format == "markmap":
                raw_content = generate_markmap(
                    extracted_text,
                    temperature=0.3,
                    max_output_tokens=4096,
                )
                content_type = "text/markdown"
                file_extension = "md"
            else:  # svg
                raw_content = generate_mindmap(
                    extracted_text,
                    temperature=0.5,
                    max_output_tokens=8192,
                )
                content_type = "image/svg+xml"
                file_extension = "svg"
        except RuntimeError as e:
            logger.error("Gemini %s mindmap generation failed for %s: %s", format, document_id, e)
            msg = str(e).lower()
            if any(k in msg for k in ("rate limit", "quota", "429")):
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Failed to generate mindmap: {e}",
                )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate mindmap: {e}",
            )

        if not raw_content or not raw_content.strip():
            logger.error("Generated %s mindmap is empty for document %s", format, document_id)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Generated {format} mindmap is empty",
            )

        # Sanitize only for SVG format
        if format == "svg":
            logger.info("Sanitizing SVG for document %s", document_id)
            try:
                final_content = sanitize_svg(raw_content)
                logger.info(
                    "SVG sanitized: %d -> %d chars",
                    len(raw_content),
                    len(final_content),
                )
            except RuntimeError as e:
                logger.error("SVG sanitization failed for %s: %s", document_id, e)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="SVG sanitization failed",
                )
        else:
            final_content = raw_content

        # Prepare storage path and filename (include document_id for uniqueness)
        mindmap_filename = f"{document_id}-mindmap.{file_extension}"
        storage_path = f"processed/{user_id}/{mindmap_filename}"
        content_bytes = final_content.encode("utf-8")

        # Upload to Supabase Storage (upsert=True for idempotency)
        try:
            upload_response = supabase.storage.from_(NOTES_BUCKET).upload(
                path=storage_path,
                file=content_bytes,
                file_options={
                    "content-type": content_type,
                    "upsert": "true",  # must be string, not boolean
                },
            )
            if hasattr(upload_response, "error") and upload_response.error:
                logger.error("Storage upload returned error (mindmap): %s", upload_response.error)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Failed to upload mindmap to storage",
                )
        except HTTPException:
            raise
        except Exception as e:  # noqa: BLE001
            logger.error("Failed to upload mindmap to storage for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to upload mindmap to storage",
            )

        # Create signed download URL
        try:
            url_response = supabase.storage.from_(NOTES_BUCKET).create_signed_url(
                storage_path, expires_in=SIGNED_URL_EXPIRY
            )
            signed_url = None
            if isinstance(url_response, dict):
                signed_url = url_response.get("signedURL") or url_response.get("signedUrl") or url_response.get("signed_url")
            else:
                signed_url = getattr(url_response, "signedURL", None)
            if not signed_url:
                raise ValueError("Signed URL missing in response")
        except Exception as e:  # noqa: BLE001
            logger.error("Failed to generate signed URL (mindmap) for %s: %s", document_id, e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate download URL",
            )

        processing_time = time.time() - start_time
        logger.info(
            "%s mindmap generation for document %s completed in %.2fs",
            format.capitalize(),
            document_id,
            processing_time,
        )

        content_preview = final_content[:500]
        # Node count heuristic based on format
        if format == "svg":
            node_count = len(re.findall(r"<(circle|rect)\b", final_content, flags=re.IGNORECASE))
        elif format == "mermaid":
            # Count lines with content (rough estimate of nodes)
            node_count = len([line for line in final_content.split("\n") if line.strip() and not line.strip().startswith("mindmap")])
        else:  # markmap
            # Count markdown headings
            node_count = len(re.findall(r"^#{1,6}\s+", final_content, re.MULTILINE))

        return GenerateMindmapResponse(
            document_id=document_id,
            filename=mindmap_filename,
            storage_path=storage_path,
            download_url=signed_url,
            format=format,
            content_preview=content_preview,
            size_bytes=len(content_bytes),
            node_count=node_count or None,
            status="success",
            message=f"{format.capitalize()} mindmap generated successfully",
            processing_time_seconds=processing_time,
        )

    except HTTPException:
        raise
    except Exception as e:  # noqa: BLE001
        logger.error("Unexpected error in /generate/mindmap: %s", e, exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred",
        )


@router.post("/flashcards", response_model=GenerateFlashcardsResponse)
def generate_flashcards_endpoint(
    document_id: UUID,
    target_count: int = Query(10, ge=1, le=50),
    user=Depends(require_user),
):
    """Generate Q&A flashcard pairs from a document's extracted text.

    Calls the Gemini API to generate flashcards in JSON format, parses and validates
    the structure, then batch inserts into the flashcards table with SM-2 initial values.

    Args:
        document_id: UUID of the document to generate flashcards from.
        target_count: Target number of flashcards (default 10, range 1-50).
        user: Authenticated user info from JWT.

    Returns:
        GenerateFlashcardsResponse with flashcard count, list, and processing time.

    Raises:
        HTTPException 404: Document not found or access denied.
        HTTPException 400: Document has no extracted text.
        HTTPException 429: Gemini API rate limit exceeded.
        HTTPException 500: Gemini API error, JSON parsing error, or database error.
    """
    start_time = time.time()

    user_id = user["sub"]
    logger.info("Generating flashcards for document %s (user %s, target=%d)", document_id, user_id, target_count)

    supabase = get_supabase_client()

    try:
        # Fetch document and verify ownership
        doc_result = (
            supabase.table("documents")
            .select("extracted_text, filename")
            .eq("id", str(document_id))
            .eq("user_id", user_id)
            .limit(1)
            .execute()
        )

        if not doc_result.data:
            logger.warning("Document %s not found for user %s", document_id, user_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found or access denied",
            )

        document = doc_result.data[0]
        extracted_text = document.get("extracted_text")

        # Validate extracted text
        if not extracted_text or not extracted_text.strip():
            logger.warning("Document %s has no extracted text", document_id)
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Document has no extracted text. Upload and extract text first.",
            )

        # Generate flashcards via Gemini
        logger.info("Generating %d flashcards for document %s", target_count, document_id)
        try:
            flashcards_json = generate_flashcards(
                extracted_text,
                temperature=0.4,
                max_output_tokens=4096,
                target_count=target_count,
            )
        except RuntimeError as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg or "quota" in error_msg or "429" in error_msg:
                logger.warning("Gemini API rate limit during flashcard generation")
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail="Gemini API rate limit exceeded. Please try again later.",
                )
            logger.error("Gemini flashcard generation failed: %s", e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate flashcards: {e}",
            )

        if not flashcards_json or not flashcards_json.strip():
            logger.error("Gemini returned empty flashcards response")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Empty flashcards generated",
            )

        # Pre-parse normalization: strip markdown code fences if present
        # Remove leading/trailing ``` and optional json language hint
        normalized_json = flashcards_json.strip()
        if normalized_json.startswith("```"):
            # Remove opening fence (```json or ```)
            normalized_json = re.sub(r"^```(?:json)?\s*", "", normalized_json)
            # Remove closing fence
            normalized_json = re.sub(r"```\s*$", "", normalized_json)
            normalized_json = normalized_json.strip()

        # Optionally extract first top-level {...} block if extra text present
        json_match = re.search(r"\{.*\}", normalized_json, re.DOTALL)
        if json_match:
            normalized_json = json_match.group(0)

        # Parse and validate JSON response
        try:
            flashcards_data = json.loads(normalized_json)
        except json.JSONDecodeError as e:
            logger.error("Invalid JSON from Gemini: %s", e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Invalid JSON format from Gemini",
            )

        # Validate structure
        if not isinstance(flashcards_data, dict) or "flashcards" not in flashcards_data:
            logger.error("Invalid flashcard JSON structure: missing 'flashcards' key")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Invalid flashcard format from Gemini",
            )

        flashcards_list = flashcards_data.get("flashcards", [])
        if not isinstance(flashcards_list, list) or len(flashcards_list) == 0:
            logger.error("Empty flashcards list in JSON response")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="No flashcards generated",
            )

        # Validate each flashcard has required fields
        for idx, fc in enumerate(flashcards_list):
            if not isinstance(fc, dict):
                logger.error("Flashcard %d is not a dict", idx)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Invalid flashcard format from Gemini",
                )
            question = fc.get("question", "").strip()
            answer = fc.get("answer", "").strip()
            if not question or not answer:
                logger.error("Flashcard %d missing question or answer", idx)
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Invalid flashcard format from Gemini",
                )

        logger.info("Generated %d flashcards for document %s", len(flashcards_list), document_id)

        # Prepare flashcard records for database insertion
        now = datetime.now(timezone.utc)
        flashcard_records = [
            {
                "user_id": user_id,
                "document_id": str(document_id),
                "question": fc["question"].strip(),
                "answer": fc["answer"].strip(),
                "efactor": DEFAULT_EFACTOR,
                "repetitions": DEFAULT_REPETITIONS,
                "interval": DEFAULT_INTERVAL,
                "next_review": now.isoformat(),
            }
            for fc in flashcards_list
        ]

        # Batch insert flashcards into database
        try:
            insert_result = supabase.table("flashcards").insert(flashcard_records).execute()
            inserted_flashcards = insert_result.data
            
            # Validate insert results include required fields
            if not inserted_flashcards:
                logger.error("Insert returned no data for flashcards")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Failed to retrieve inserted flashcards from database",
                )
            
            # Verify each row has id and created_at
            for idx, fc in enumerate(inserted_flashcards):
                if "id" not in fc or "created_at" not in fc:
                    logger.error("Inserted flashcard %d missing id or created_at", idx)
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail="Database insert returned incomplete data",
                    )
                    
        except HTTPException:
            raise
        except Exception as e:  # noqa: BLE001
            logger.error("Failed to insert flashcards into database: %s", e, exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to save flashcards to database",
            )

        # Build response
        processing_time = time.time() - start_time
        flashcard_responses = [FlashcardResponse(**fc) for fc in inserted_flashcards]

        return GenerateFlashcardsResponse(
            document_id=document_id,
            flashcard_count=len(inserted_flashcards),
            flashcards=flashcard_responses,
            status="success",
            message="Flashcards generated successfully",
            processing_time_seconds=processing_time,
        )

    except HTTPException:
        raise
    except Exception as e:  # noqa: BLE001
        logger.error("Unexpected error in /generate/flashcards: %s", e, exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred",
        )
